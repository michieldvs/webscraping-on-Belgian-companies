{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GAfkSO8WX0vq"},"outputs":[],"source":["!pip install requests\n","!pip install nltk\n","!pip install bs4\n","!pip install pandas\n","!pip install numpy\n","!pip install multiprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIPup5yFv_Ih"},"outputs":[],"source":["import requests\n","import nltk\n","import string \n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import numpy as np\n","import urllib.parse\n","import csv\n","import json\n","import multiprocessing\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.util import ngrams\n","from urllib.parse import urlparse\n","from google.colab import drive\n","from multiprocessing import Pool\n","from multiprocessing import Pool"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yd35scUCa4h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682972047987,"user_tz":-120,"elapsed":33281,"user":{"displayName":"Michiel Devos","userId":"09343380437438572020"}},"outputId":"191d93ef-ff93-4f31-fa22-863e5358ac5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at Drive\n"]}],"source":["drive.mount('Drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yHsRpw1kswA-"},"outputs":[],"source":["!pip install lingua-language-detector\n","from lingua import Language, LanguageDetectorBuilder\n","languages = [Language.ENGLISH, Language.FRENCH, Language.GERMAN, Language.DUTCH]\n","detector = LanguageDetectorBuilder.from_languages(*languages).build()\n","\n","from dateutil.parser import parse\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","cookies = ['accepteer', 'accepteren', 'accept', 'advertenties', 'advertisements', 'analyse', 'analyze', 'analytics',\n","            'analytische','analytisch','bepaalde', 'certain', 'bezoeker','visitor','belgian', 'browser','cookies','cookie','copyright',\n","            'choose','kies','kiezen', 'delen', 'share','derden','third', 'parties','party', 'disclaimer', 'functioneel','functional',\n","            'functionele','functioneren','function','gebruik', 'gebruiker','gebruikt', 'use', 'user','used','inhoud',\n","            'content','instellingen','settings', 'klikken','click','login','register','registreer','necessary','noodzakelijk', \n","            'noodzakelijke','opgeslagen', 'opslaan', 'save','saved','pagina','page','policy','beleid','privacy', 'relevante',\n","            'relevant','social','sociaal','sociale', 'store','opslaan','toestemming','consent', 'voorkeuren','preference',\n","            'preferences','website', 'websites','algemene','voorwaarden', 'aanmelden','account','gegevens', 'www', 'com', \n","            'contact', 'contacteer','websites','gebruiken','this', 'these', 'that', 'cookieverklaring', 'toggle', 'more', 'about', 'rights',\n","            'reserved', 'privacy', 'support','ondersteuning', 'copyright', 'cookiebeleid']\n","\n","#Languages + others\n","others = ['languages', 'talen', 'taal', \n","          'select', 'selecteer', 'selecteren', 'united', 'kingdom', 'verenigd', 'koninkrijk', 'afspraak', 'maandag', 'dinsdag', 'woensdag',\n","          'donderdag', 'vrijdag', 'zaterdag', 'zondag', 'gesloten', 'openingsuren', 'instagram', 'youtube', 'linkedin', 'twitter', 'facebook',\n","          'januari', 'februari', 'maart', 'april', 'mei', 'juni', 'juli', 'augustus', 'september', 'oktober', 'november', 'december', 'afspraak',\n","          'meeting', 'appointment', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'openingsuren', 'opening', 'hours',\n","          'gesloten', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'oktober', 'november', 'december', 'vlaanderen',\n","          'skip', 'main', 'menu', 'subscribe', 'subscribing', 'open', 'new', 'window', 'subscribing', 'open', 'new', 'window', 'close', 'search', 'login',\n","          'register', 'cart', 'checkout','product', 'products', 'shop', 'store', 'buy', 'purchase', 'order', 'price', 'sale', 'discount', 'shipping', 'return',\n","          'terms', 'privacy', 'policy', 'about', 'contact', 'faq', 'help', 'support', 'service', 'news', 'blog', 'article', 'read','watch', 'listen', 'download',\n","          'upload', 'share', 'follow', 'like', 'comment', 'share', 'explore', 'discover', 'find','learn', 'sign', 'up', 'back', 'home', 'page', 'site', 'error', '404', '500',\n","          'internal', 'server', 'maintenance','under', 'construction', 'coming', 'soon', 'not', 'available', 'offline', 'online', 'click', 'here', 'there', 'now','today', \n","          'yesterday', 'tomorrow', 'week', 'month', 'year', 'hour', 'minute', 'second', 'first', 'last', 'next','previous', 'previous', 'top', 'bottom', 'left', 'right',\n","          'center', 'east', 'west', 'north', 'south', 'up', 'down','in', 'out', 'over', 'under', 'around', 'through', 'above', 'below', 'before', 'after', 'during', 'while',\n","          'because','why', 'how', 'what', 'where', 'when', 'who', 'which', 'that', 'these', 'those', 'this', 'that', 'here', 'there','deutsch', 'duits', 'german', 'germany', 'dutch', 'nederlands', 'engels', \n","          'english', 'francais', 'français', 'frans', 'french', 'france', 'belgië', 'belgie', 'belgium', 'nederland', 'netherlands', 'duitsland', 'espanol', 'italian', 'italy',\n","          'spanish', 'spain', 'portuguese', 'portugal', 'danish', 'denmark', 'swedish', 'sweden', 'norwegian', 'norway', 'finnish', 'finland',\n","          'polish', 'poland', 'czech', 'czech republic', 'slovak', 'slovakia', 'language', 'benelux', 'afghanistan', 'albania', 'algeria', 'andorra',\n","          'angola', 'antigua and barbuda', 'argentina', 'armenia', 'australia', 'austria', 'azerbaijan', 'bahamas', 'bahrain', 'bangladesh', 'barbados',\n","          'belarus', 'belgium', 'belize', 'benin', 'bhutan', 'bolivia', 'bosnia and herzegovina', 'botswana', 'brazil', 'brunei', 'bulgaria', 'burkina faso',\n","          'burundi', 'cabo verde', 'cambodia', 'cameroon', 'canada', 'central african republic', 'chad', 'chile', 'china', 'colombia', 'comoros',\n","          'congo, republic of the', 'congo, democratic republic of the', 'costa rica', 'cote d\\'ivoire', 'croatia', 'cuba', 'cyprus', 'czech republic', 'denmark',\n","          'djibouti', 'dominica', 'dominican republic', 'ecuador', 'egypt', 'el salvador', 'equatorial guinea', 'eritrea', 'estonia', 'eswatini', 'ethiopia', 'fiji',\n","          'finland', 'france', 'gabon', 'gambia', 'georgia', 'germany', 'ghana', 'greece', 'grenada', 'guatemala', 'guinea', 'guinea-bissau', 'guyana', 'haiti',\n","          'honduras', 'hungary', 'iceland', 'india', 'indonesia', 'iran', 'iraq', 'ireland', 'israel', 'italy', 'jamaica', 'japan', 'jordan', 'kazakhstan', 'kenya', \n","          'kiribati', 'kosovo', 'kuwait', 'kyrgyzstan', 'laos', 'latvia', 'lebanon', 'lesotho', 'liberia', 'libya', 'liechtenstein', 'lithuania', 'luxembourg',\n","          'madagascar', 'malawi', 'malaysia', 'maldives', 'mali', 'malta', 'marshall islands', 'mauritania', 'mauritius', 'mexico', 'micronesia', 'moldova',\n","          'monaco', 'mongolia', 'montenegro', 'morocco', 'mozambique', 'myanmar (burma)', 'namibia', 'nauru', 'nepal', 'netherlands', 'new zealand', 'nicaragua',\n","          'niger', 'nigeria', 'north korea', 'north macedonia', 'norway', 'oman', 'pakistan', 'palau', 'panama', 'papua new guinea', 'paraguay', 'peru', 'philippines',\n","          'poland', 'portugal', 'qatar', 'romania', 'russia', 'rwanda', 'saint kitts and nevis', 'saint lucia', 'saint vincent and the grenadines', 'samoa', 'san marino',\n","          'sao tome and principe', 'saudi arabia', 'senegal', 'serbia', 'seychelles', 'sierra leone', 'singapore', 'slovakia', 'slovenia']\n","\n","\n","\n","#punctuations\n","punctuations = list(string.punctuation)\n","punctuations.append(\"|\")\n","punctuations.append(\"€\")\n","punctuations.append(\"$\")\n","\n","def preprocessing(list_companies_text, stopwords_list, language, cookies, other, punctuations):\n","  #Removing stopwords\n","  list_companies_text = [word for word in list_companies_text if word not in stopwords_list]\n","  #Removing cookies\n","  list_companies_text = [word for word in list_companies_text if word not in cookies]\n","  #Removing other words\n","  list_companies_text = [word for word in list_companies_text if word not in other]\n","  #Removing dates\n","  for word in list_companies_text:\n","    try: \n","      parse(word, fuzzy=False)\n","      list_companies_text.remove(word)\n","    except:\n","      continue\n","  #Lemmatization for English\n","  if language == \"english\":\n","    lemmatize = WordNetLemmatizer()\n","    list_companies_text = [lemmatize.lemmatize(word) for word in list_companies_text]\n","  #Stemming for Dutch, French & German\n","  if (language in ['dutch', 'french', 'german']):\n","    stem_ = SnowballStemmer(language)\n","    list_companies_text = [stem_.stem(word) for word in list_companies_text]\n","  #Removing punctuations\n","  for word in list_companies_text:\n","    for letter in word:\n","      if letter in punctuations:\n","        list_companies_text.remove(word)\n","        break\n","  list_companies_text = [word for word in list_companies_text if word not in punctuations]\n","  #Removing numbers\n","  for word in list_companies_text:\n","    try:\n","      float(word)\n","      list_companies_text.remove(word)\n","    except:\n","      continue\n","  #Return the prepocessed text in a document format\n","  return \" \".join(list_companies_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81XkHpoLATEJ"},"outputs":[],"source":["def languageANDprep(text):\n","  stopwords_list = []\n","  language = \"\"\n","  if detector.detect_language_of(text) == Language.DUTCH:\n","    stopwords_list = stopwords.words('dutch')\n","    language = \"dutch\"\n","  elif detector.detect_language_of(text) == Language.ENGLISH:\n","    stopwords_list = stopwords.words('english')\n","    language = \"english\"\n","  elif detector.detect_language_of(text) == Language.FRENCH:\n","    stopwords_list = stopwords.words('french')\n","    language = \"french\"\n","  elif detector.detect_language_of(text) == Language.GERMAN:\n","    stopwords_list = stopwords.words('german')\n","    language = \"german\"\n","  words_list = word_tokenize(text.lower())\n","  #print(words_list)\n","  return preprocessing(words_list, stopwords_list, language = language, cookies = cookies, other = others, punctuations=punctuations)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQzk3C4ibDQj"},"outputs":[],"source":["def get_urls_on_page_multi(url, depth=1):\n","  urls = []\n","  headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.3 Safari/605.1.15'}\n","  tempurls = [url]\n","  tempurlsprevious = []\n","  tempurlscontent = []\n","    \n","  # scraping the links for all the 'href' links and preprocess them\n","  for pages in range(depth):\n","    currenturls = [j for j in tempurls if j not in tempurlsprevious]\n","    tempurlsprevious = tempurls.copy()\n","    for url in currenturls:\n","      try:\n","        r = requests.get(url, timeout = 5, headers=headers)\n","      except:\n","        continue\n","      soup = BeautifulSoup(r.content, 'html.parser') \n","      for link in soup.find_all('a'):\n","        templink = link.get('href')\n","        if templink == None:\n","          continue\n","        elif (templink.endswith(\".pdf\") or templink.endswith(\".PDF\")):\n","          continue\n","        #for GBL, this is necessary\n","        elif templink.endswith(\"zip\") or templink.endswith(\"esef\"):\n","          continue\n","        elif ((\"file\" or \"files\" or \"document\" or \"mailto\") in templink):\n","          continue\n","        elif templink.startswith(\"/\"):\n","          templink = url + templink\n","        elif not (templink.startswith(\"https://\") or templink.startswith(\"www\") or templink.startswith(\"http\")):\n","          templink = url + \"/\" + templink\n","\n","        #Delete duplicate webpages, (can maybe also be done with drop_duplicates())\n","        try:\n","          tempr = requests.get(templink, timeout = 5, headers=headers)\n","          tempcontent = BeautifulSoup(tempr.content, 'html.parser')\n","        except:\n","          break\n","        if (tempcontent in tempurlscontent):\n","          templink = \"//.//\"\n","        else:\n","          tempurlscontent.append(tempcontent)\n","\n","        #check if the links are from the same domain as the company, that they are actually created by the company\n","        if urlparse(url).netloc == urlparse(templink).netloc:\n","          tempurls.append(templink)\n","        elif urlparse(templink).netloc.split(\".\")[1] == urlparse(url).netloc.split(\".\")[1]:\n","          tempurls.append(templink)\n","        elif urlparse(templink).netloc.split(\".\")[1] == urlparse(url).netloc.split(\".\")[0]:\n","          tempurls.append(templink)\n","  return tempurls\n","\n","def get_all_urls_on_page_depth_multi(givenurls, depth=1):\n","  allurls = []\n","  with multiprocessing.Pool(processes=2) as pool:\n","    for i in givenurls:\n","      allurls.append(pool.apply_async(get_urls_on_page_multi, args=(i, depth)).get())\n","  return allurls\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-bACTXOIX-P"},"outputs":[],"source":["def process_url(url):\n","    try:\n","        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.3 Safari/605.1.15'}\n","        response = requests.get(url, headers=headers)\n","        soup = BeautifulSoup(response.content, \"html.parser\", from_encoding=response.encoding)\n","        text = soup.get_text(\" \", strip=True)\n","        return languageANDprep(text)\n","    except:\n","        return \"process_urls failed\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwIPQ3nHIa5m"},"outputs":[],"source":["def get_content(names, dataurls, outputname, depth=1):\n","  dataurlsused = []\n","  for i in dataurls:\n","    if not i.startswith(\"http\"):\n","      dataurlsused.append(\"http://\" + i)\n","    else:\n","      dataurlsused.append(i)\n","  allurls = get_all_urls_on_page_depth_multi(dataurlsused, depth)\n","  jsonempty = {}\n","  filename = '/Drive/MyDrive/' + outputname + '.json'\n","  with open(filename, 'w') as json_file:\n","    json.dump(jsonempty, json_file, indent=4)\n","\n","  # send a GET request to the URL and extract the text\n","  documentlist = {}\n","  if __name__ == '__main__':\n","    pool = Pool(processes=1)\n","    for j in allurls:\n","      urlss = j\n","      filtered_documents = pool.map(process_url, urlss)\n","      with open(filename, 'r') as json_file:\n","        newjsonfile = json.load(json_file)\n","      newjsonfile[names[dataurlsused.index(j[0])]] = filtered_documents\n","      with open(filename, 'w') as json_file:\n","        json.dump(newjsonfile, json_file, indent=4)\n","    pool.close()\n","    pool.join()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ob4idS0eMceX"},"outputs":[],"source":["# Set the file path\n","file_path = '/content/Drive/My Drive/Thesis/Input_scraping/Bel-first_Export_ALL.xlsx'\n","\n","# Load the Excel file into a pandas dataframe\n","df = pd.read_excel(file_path)\n","\n","# Create empty lists for the name and website address columns\n","names = []\n","websites = []\n","\n","# Iterate over the rows in the dataframe and append the name and website address to their respective lists\n","for index, row in df.iterrows():\n","    names.append(row[\"Naam\"])\n","    websites.append(row[\"Web adres\"])\n","\n","# Print the lists to check that they contain the correct values\n","print(names)\n","print(websites)\n","\n","get_content(names, websites, \"Belgie_ALL\", depth=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Z04Wbz8mMQs"},"outputs":[],"source":["# Set the file path\n","file_path = '/content/drive/MyDrive/Thesis/Input_scraping/Bel-first_Export_top250.xlsx'\n","\n","# Load the Excel file into a pandas dataframe\n","df = pd.read_excel(file_path)\n","\n","# Create empty lists for the name and website address columns\n","names = []\n","websites = []\n","\n","# Iterate over the rows in the dataframe and append the name and website address to their respective lists\n","for index, row in df.iterrows():\n","  names.append(row[\"Name\"])\n","  websites.append(row[\"Website address\"])\n","\n","# Print the lists to check that they contain the correct values\n","print(names)\n","print(websites)\n","\n","get_content(names, websites, \"Belgie_Top250_depth1\", depth=1)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}